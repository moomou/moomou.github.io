<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Learning to Debug Gibberish - moomou</title><link rel="icon" type="image/png" href=favicon.ico /><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:url" content="/posts/2023-12-27-ohmy/">
  <meta property="og:site_name" content="moomou">
  <meta property="og:title" content="Learning to Debug Gibberish">
  <meta property="og:description" content="Overview # Recently, I went down a rabbit hole of debugging opensource LLM spewing gibberish on my PC. The investigation was the one of the most difficult (but interesting) debugging experience I have encountered so far.
This blog will chronicle the problem I encountered, debugging steps I took, and finally, the resolution and workaround I adopted.
tl;dr I traced down the source of LLM spewing gibberish to faulty RAM sticks after getting different sha256 checksums on a static file on different runs. I was then able to salvage ~40% of the faulty RAM stick capacity after discovering Linux kernl parameter memtest=.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-12-27T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-12-27T00:00:00+00:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Learning to Debug Gibberish">
  <meta name="twitter:description" content="Overview # Recently, I went down a rabbit hole of debugging opensource LLM spewing gibberish on my PC. The investigation was the one of the most difficult (but interesting) debugging experience I have encountered so far.
This blog will chronicle the problem I encountered, debugging steps I took, and finally, the resolution and workaround I adopted.
tl;dr I traced down the source of LLM spewing gibberish to faulty RAM sticks after getting different sha256 checksums on a static file on different runs. I was then able to salvage ~40% of the faulty RAM stick capacity after discovering Linux kernl parameter memtest=.">
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" /><link rel="stylesheet" type="text/css" href="/css/dark.css"  />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="/js/main.js"></script>
	<script src="/js/abc.js"></script>
	<script src="/js/xyz.js"></script>
	<script src="https://code.jquery.com/jquery-3.4.1.js"></script>
	
	<link rel="stylesheet" type="text/css" href="/css/anchors.css" />

</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title"><a href="/">moomou</a></h1>
	<div class="site-description"><h2>(ﾉ≧∇≦)ﾉ ﾐ ┸┸</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/moomou" title="Github"><i data-feather="github"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">All posts</a>
			</li>
			
			<li>
				<a href="https://go.mou.dev/resume">Resume</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Learning to Debug Gibberish</h1>
      
      <div class="meta">Posted at &mdash; Dec 27, 2023</div>
      
      
		</div>

    

		<div class="markdown">
			<h2 id="overview">
  Overview
  <a href="#overview" class="heading-anchor">#</a>
</h2>
<p>Recently, I went down a rabbit hole of debugging opensource LLM spewing gibberish on my PC.
The investigation was the one of the most difficult (but interesting) debugging experience I have encountered so far.</p>
<p>This blog will chronicle the problem I encountered, debugging steps I took, and finally, the resolution and workaround I adopted.</p>
<blockquote>
<p><strong>tl;dr</strong> I traced down the source of LLM spewing gibberish to faulty RAM sticks after getting different sha256 checksums on a static file on different runs. I was then able to salvage ~40% of the faulty RAM stick capacity after discovering Linux kernl parameter <code>memtest=</code>.</p>
</blockquote>
<h2 id="the-problem">
  The Problem
  <a href="#the-problem" class="heading-anchor">#</a>
</h2>
<p>Like everyone else, I have been spending a lot of time studying and playing with opensource LLMs, primarily leveraging my workstation sitting in my garage.</p>
<p>I focused my time originally on deploying and setting up a local instance with <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> and <a href="https://github.com/vllm-project/vllm">vllm </a> using quantized Mistral models.</p>
<p>However, as I delved deeper and started customizing models (another blog post!) using pytorch and <a href="https://huggingface.co/docs/transformers/index">transformers</a> library, I noticed that models I downloaded directly from Huggingface all spew gibberish!</p>
<p>This problem showed up in essentially all the models I tried without any obvious patterns. To make the whole situation extra confusing, <em>some</em> models <em>work</em>.</p>
<h2 id="debugging">
  Debugging
  <a href="#debugging" class="heading-anchor">#</a>
</h2>
<p>The initial suspicion I had was on some subtle software compatibilities issues in my setup. After all, getting a LLM to run locally requires an insane amount of software.</p>
<p>So as a first step, I started by upgrading and downgradign Nvidia driver and CUDA versions. Specifically, I tried different combinations of CUDA 12.1 and 11.8 with driver versions ranging from 520 to 535.</p>
<p>Concurrently, I also tried different combinations of python, pytorch, and transformers versions.</p>
<p>However, all combinations led to the same results - LLM still spewed gibberish.</p>
<p>Googling (and asking ChatGPT) on various queries (<code>&quot;transformers+all+models+gibberish&quot;</code>, <code>&quot;nvidia+3090+pytorch+gibberish&quot;</code>, <code>&quot;qwen+gibberish&quot;</code>, etc.) led to nowhere.</p>
<h2 id="verifying-in-cloud">
  Verifying in Cloud
  <a href="#verifying-in-cloud" class="heading-anchor">#</a>
</h2>
<p>After trying just about all possible combinations of software dependencies, I decided I needed a working setup to find out what is wrong with my workstation.</p>
<p>I hopped over to <a href="https://colab.research.google.com">Google Colab</a> and ran <a href="https://huggingface.co/Qwen/Qwen-7B">Qwen-7B</a>&rsquo;s starter template</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers.generation <span style="color:#f92672">import</span> GenerationConfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;Qwen/Qwen-7B-Chat&#34;</span>, trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;Qwen/Qwen-7B-Chat&#34;</span>, device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>, trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response, history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>chat(tokenizer, <span style="color:#e6db74">&#34;你好&#34;</span>, history<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)
</span></span><span style="display:flex;"><span>print(response)
</span></span></code></pre></div><p>and sure enough, it worked on Google cloud!</p>
<p>I then noted down the Nvidia driver version (<code>!nvidia-smi</code>) and python dependencies (<code>!pip3 freeze</code>) and worked toward replicating that environment locally. Surely if it worked in the cloud it must work on my PC too!</p>
<p>Except it did not&hellip;</p>
<h2 id="sha256-in-a-loop">
  SHA256 in a Loop
  <a href="#sha256-in-a-loop" class="heading-anchor">#</a>
</h2>
<p>Despite meticulously replicating the software environment from Colab, LLM still talked gibberish. Running out places to look, I decided to verify the model weight files (safetensors, bin files, etc.) are correct - ie have the same checksum as on Google Colab.</p>
<p>The possibility of model files being corrupted somehow is quite unlikely since I tested dozens of different models and pytorch modules <code>`load`</code> calls were all succeeding.</p>
<p>However, it is at this point that I am reminded of the following quote</p>
<blockquote>
<p>Once you eliminate the impossible, whatever remains, no matter how improbable, must be the truth.</p>
</blockquote>
<p>So I grabbed the sha256sum of the <code>chatglm-6B</code> from Google Colab and compared them against my local copy and sure enough the checksums were different.</p>
<p>&ldquo;Ok so I have some corrupted files. I will just download them again&rdquo;, I thought to myself.</p>
<p>I duly downloaded the model files again and ran <code>sha256sum</code> on the new files, only to discover they were still different from Colab&rsquo;s checksums AND from previous downloaded files. That is, I have 3 set of checksums at this point!</p>
<p>A little confused, I reran the checkums again on the newly downloaded files and this time yieled completely new sha256 checksums, different from all previous attempts.</p>
<h2 id="faulty-hardware">
  Faulty Hardware
  <a href="#faulty-hardware" class="heading-anchor">#</a>
</h2>
<p>At this point, I realized I have a <strong>hardware</strong> problem.</p>
<p>The prime suspects were hard drive, network, and/or memory.</p>
<p>I eliminated hard drive by downloading files to a different drive and still the checksums were different.</p>
<p>This is replicable irrespective of file&rsquo;s physical location, whether SATA drives, M.2 sticks, or external USB drive. The faulty hardware is <strong>memory</strong>.</p>
<p>What turned out to be happening is even when model files were downloaded correctly, when model weights were loaded into memory for LLM inference, they were silently corrupted due to bad RAM so every model spewed gibberish!!</p>
<h2 id="linux-kernel-to-the-rescue">
  Linux Kernel to the Rescue
  <a href="#linux-kernel-to-the-rescue" class="heading-anchor">#</a>
</h2>
<p>After testing my RAM sticks one by one on each of the motherboard slot, I was able to identify the faulty RAM sticks.</p>
<p>While I was glad to have my sanity restored, I was bummed about losing 64GB of RAM capacity.</p>
<p>What was interesting was the RAM <em>mostly</em> worked - the data corruption was rare enough that it only manifested itself on LLM inference!</p>
<p>I researched on the possibility of identifying the bad memory regions and whether Linux can cope with bad RAM somehow.</p>
<p>The answere turned out to be YES and it is simple to enable as well.</p>
<p>All it takes is adding a kernel parameter <code>memtest=N</code> to instruct Linux kernel to perform <code>N</code> memory test before boot to cordon off any identified bad memory regions.</p>
<pre tabindex="0"><code>memtest=        [KNL,X86,ARM] Enable memtest
                        Format: &lt;integer&gt;
                        default : 0 &lt;disable&gt;
                        Specifies the number of memtest passes to be
                        performed. Each pass selects another test
                        pattern from a given set of patterns. Memtest
                        fills the memory with this pattern, validates
                        memory contents and reserves bad memory
                        regions that are detected.
</code></pre><p>This parameter can be added to <code>/etc/default/grub</code> so Linux boots with memory test.</p>
<p>Here is my updated grub file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># If you change this file, run &#39;update-grub&#39; afterwards to update</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># /boot/grub/grub.cfg.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># For full documentation of the options in this file, see:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   info -f grub -n &#39;Simple configuration&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>GRUB_DEFAULT<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>GRUB_TIMEOUT_STYLE<span style="color:#f92672">=</span>menu
</span></span><span style="display:flex;"><span>GRUB_TIMEOUT<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>GRUB_DISTRIBUTOR<span style="color:#f92672">=</span><span style="color:#e6db74">`</span>lsb_release -i -s 2&gt; /dev/null <span style="color:#f92672">||</span> echo Debian<span style="color:#e6db74">`</span>
</span></span><span style="display:flex;"><span>GRUB_CMDLINE_LINUX_DEFAULT<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;memtest=4&#34;</span>
</span></span><span style="display:flex;"><span>GRUB_CMDLINE_LINUX<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Uncomment to enable BadRAM filtering, modify to suit your needs</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This works with Linux (no patch required) and with any kernel that obtains</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#GRUB_BADRAM=&#34;0x01234567,0xfefefefe,0x89abcdef,0xefefefef&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Uncomment to disable graphical terminal (grub-pc only)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#GRUB_TERMINAL=console</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The resolution used on graphical terminal</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># note that you can use only modes which your graphic card supports via VBE</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># you can see them in real GRUB with the command `vbeinfo&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#GRUB_GFXMODE=640x480</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Uncomment if you don&#39;t want GRUB to pass &#34;root=UUID=xxx&#34; parameter to Linux</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#GRUB_DISABLE_LINUX_UUID=true</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Uncomment to disable generation of recovery mode menu entries</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#GRUB_DISABLE_RECOVERY=&#34;true&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Uncomment to get a beep at grub start</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#GRUB_INIT_TUNE=&#34;480 440 1&#34;</span>
</span></span></code></pre></div><p>Notice BadRAM is mentioned as well if you happen to have the bad regions identified via another tool such as MemTest86. I did not pursue this option since running memory test on boot is more dynamic and easier.</p>
<p>Leveraging <code>memtest</code>, Linux determined 40% of the faulty RAM sticks is still usable!
I also reran my sha256sum test a good number of times to verify stability.</p>
<img style="float: right" src="/sad_happy.png" alt="drawing" width="64"/>
<br/>
<br/>
		</div>

		<div class="post-tags">
			
				
			
		</div>
		</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> since 2017 </div>
	</nav>
</div>


<script async src="https://www.googletagmanager.com/gtag/js?id=trackingcode"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'trackingcode');
</script><script>feather.replace()</script>
</body>
</html>
